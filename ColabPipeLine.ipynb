{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "ls -1 /content/drive/MyDrive/AI_Rayan/P2/Q1/Dataset/train/images | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPR2HTR159M",
        "outputId": "37a9f179-2bd7-4c0e-d72e-ea8e8eed168c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -1 /content/drive/MyDrive/AI_Rayan/P2/Q1/Dataset/val/images | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbC-tWCo2Djf",
        "outputId": "9937b237-6b76-485f-e060-6795a6ea4920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDPca9r7yJm7"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMULh_95gtTQ"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#%env CUDA_VISIBLE_DEVICES=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rYgRJEmgvKz",
        "outputId": "67ab08a6-5768-4b52-fc36-7bfa895db4ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Collecting noisereduce\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from noisereduce) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from noisereduce) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from noisereduce) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from noisereduce) (4.66.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from noisereduce) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.16.0)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-3.0.3\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n"
          ]
        }
      ],
      "source": [
        "colab_requirements = [\n",
        "    \"pip install librosa\",\n",
        "    \"pip install noisereduce\",\n",
        "    \"pip install soundfile\",\n",
        "\n",
        "]\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "def run_subprocess_command(cmd):\n",
        "    # run the command\n",
        "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
        "    # print the output\n",
        "    for line in process.stdout:\n",
        "        print(line.decode().strip())\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    for i in colab_requirements:\n",
        "        run_subprocess_command(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6K-iwu8f8ol",
        "outputId": "37c0f69f-62a5-4497-e214-22f2377c4bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SD4yF1PZRrP"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q79q75HJado-",
        "outputId": "8befebaf-fb34-405c-f271-5214462f1cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upbCqx6eSuRY"
      },
      "source": [
        "# Preprocessing - Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "JV8AonPzg_wx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Vru2Qs_hAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7YMGU4LfYQ"
      },
      "source": [
        "## Dataset (TheDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w06ggeGRSwNo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from functools import cmp_to_key\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "class TheDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir,\n",
        "        chunk_size,\n",
        "        sample_rate,\n",
        "        exclude_list=None,\n",
        "        transform=None,\n",
        "        with_labels=True\n",
        "    ):\n",
        "        self.root_dir = root_dir\n",
        "        self.chunk_size = chunk_size\n",
        "        self.sample_rate = sample_rate\n",
        "        self.transform = transform\n",
        "        self.with_labels = with_labels\n",
        "        self.dataset = []\n",
        "\n",
        "        # If exclude_list is not provided, initialize it as an empty list\n",
        "        if exclude_list is None:\n",
        "            exclude_list = []\n",
        "\n",
        "        # Get all subdirectories excluding those in the exclude_list\n",
        "        subdirs = sorted([d for d in os.listdir(root_dir)\n",
        "                          if os.path.isdir(os.path.join(root_dir, d)) and d not in exclude_list])\n",
        "\n",
        "        # Initialize lists to store all PNG filenames\n",
        "        all_pngs = []\n",
        "\n",
        "        # Iterate through subdirectories\n",
        "        for subdir in subdirs:\n",
        "            cur_fol = os.path.join(self.root_dir, subdir)\n",
        "            png_files = self.sort_files([f for f in os.listdir(cur_fol) if f.lower().endswith('.png')])\n",
        "            png_files = [os.path.join(cur_fol, f) for f in png_files]\n",
        "\n",
        "            rewrited = self.rewrite(png_files)\n",
        "            all_pngs.extend(rewrited)\n",
        "\n",
        "        self.dataset = all_pngs\n",
        "\n",
        "\n",
        "    def downsample_labels(self, labels, num_seconds, sample_rate, threshold=0.05):\n",
        "      downsampled_labels = np.zeros(num_seconds, dtype=int)  # Initialize output array\n",
        "\n",
        "      for i in range(num_seconds):\n",
        "          start_index = i * sample_rate\n",
        "          end_index = start_index + sample_rate\n",
        "\n",
        "          # Get the current second's labels\n",
        "          current_second_labels = labels[start_index:end_index]\n",
        "\n",
        "          # Check if more than 5% of the samples are 1s\n",
        "          if np.mean(current_second_labels) > threshold:\n",
        "              downsampled_labels[i] = 1  # Flag as 1 if over threshold\n",
        "      return downsampled_labels\n",
        "\n",
        "    def rewrite(self, array):\n",
        "      output = []\n",
        "      for i in range(len(array)-2):\n",
        "        mini = array[i:i+3]\n",
        "        output.append(mini)\n",
        "\n",
        "      return output\n",
        "\n",
        "\n",
        "\n",
        "    def sort_files(self, file_list):\n",
        "        def extract_number(filename):\n",
        "            match = re.match(r'chunk_(\\d+)\\.png$', filename.lower())\n",
        "            return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "        def custom_key(file1, file2):\n",
        "            num1 = extract_number(file1)\n",
        "            num2 = extract_number(file2)\n",
        "            if num1 == num2:\n",
        "                return file1.lower().compare(file2.lower())\n",
        "            return num1 - num2\n",
        "\n",
        "        return sorted(file_list, key=cmp_to_key(custom_key))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.dataset[idx]\n",
        "        img_before = chunk[0]\n",
        "        img_current = chunk[1]\n",
        "        img_after = chunk[2]\n",
        "\n",
        "        label_name_current = chunk[1][:-4] + \".npy\"\n",
        "        label_current = np.load(label_name_current)\n",
        "\n",
        "        mel_img_before = Image.open(img_before).convert('RGB')  # Convert to RGB for CNN\n",
        "        mel_img_current = Image.open(img_current).convert('RGB')  # Convert to RGB for CNN\n",
        "        mel_img_after = Image.open(img_after).convert('RGB')  # Convert to RGB for CNN\n",
        "\n",
        "\n",
        "        # Apply any transformations (e.g., resizing, normalization)\n",
        "        if self.transform:\n",
        "            mel_img_before = self.transform(mel_img_before)\n",
        "            mel_img_current = self.transform(mel_img_current)\n",
        "            mel_img_after = self.transform(mel_img_after)\n",
        "\n",
        "\n",
        "        if self.with_labels:\n",
        "          labels = self.downsample_labels(label_current, self.chunk_size, self.sample_rate)\n",
        "          labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "          return (mel_img_before,mel_img_current,mel_img_after), labels\n",
        "\n",
        "        return (mel_img_before,mel_img_current,mel_img_after)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgMiIp31Qf1s"
      },
      "source": [
        "# Model (TheModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9BhJfoZQfIR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the Mini ResNet for each image\n",
        "class MiniResNet(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(MiniResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out\n",
        "\n",
        "# Define the final network that processes 3 images separately and concatenates the outputs\n",
        "class TheModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(TheModel, self).__init__()\n",
        "\n",
        "        # The MiniResNet used to process each individual image\n",
        "        self.resnet_branch1 = MiniResNet(num_labels)\n",
        "        self.resnet_branch2 = MiniResNet(num_labels)\n",
        "        self.resnet_branch3 = MiniResNet(num_labels)\n",
        "\n",
        "        # Fully connected layer after concatenating outputs from 3 branches\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * 3, 1024),  # 256 channels from each ResNet branch, concatenated for 3 images\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_labels),  # Output final predictions\n",
        "            nn.Sigmoid()  # Use Sigmoid for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, img1, img2, img3):\n",
        "        # Pass each image through the shared ResNet branch\n",
        "        out1 = self.resnet_branch1(img1)\n",
        "        out2 = self.resnet_branch2(img2)\n",
        "        out3 = self.resnet_branch3(img3)\n",
        "\n",
        "        # Global average pooling for each branch\n",
        "        out1 = F.adaptive_avg_pool2d(out1, (1, 1))\n",
        "        out2 = F.adaptive_avg_pool2d(out2, (1, 1))\n",
        "        out3 = F.adaptive_avg_pool2d(out3, (1, 1))\n",
        "\n",
        "        # Flatten the outputs\n",
        "        out1 = out1.view(out1.size(0), -1)  # (batch_size, 256)\n",
        "        out2 = out2.view(out2.size(0), -1)  # (batch_size, 256)\n",
        "        out3 = out3.view(out3.size(0), -1)  # (batch_size, 256)\n",
        "\n",
        "        # Concatenate the outputs from the 3 images\n",
        "        out = torch.cat([out1, out2, out3], dim=1)  # (batch_size, 256 * 3)\n",
        "\n",
        "        # Pass through the fully connected layers\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqa5-nnuGNfo"
      },
      "source": [
        "# Main Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2tuqSiRHD6t"
      },
      "source": [
        "## Pipeline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtrEO_DAHIgf"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, Optional\n",
        "\n",
        "class PipelineSector(ABC):\n",
        "  def __init__(self, input_is_list=False, ignore_input=False):\n",
        "    self.input_is_list = input_is_list\n",
        "    self.ignore_input = ignore_input\n",
        "\n",
        "  @abstractmethod\n",
        "  def __call__(self, input: Optional[Any]) -> Any:\n",
        "    pass\n",
        "\n",
        "class Pipeline(object):\n",
        "  def __init__(self):\n",
        "    self.pipe_list = []\n",
        "  def append(self, sector:PipelineSector):\n",
        "    self.pipe_list.append(sector)\n",
        "\n",
        "  def run(self, x:Any) -> Any:\n",
        "    for sector in self.pipe_list:\n",
        "      print(f'[{sector.__class__.__name__}] Is Running')\n",
        "      if sector.ignore_input:\n",
        "        y = sector(None)\n",
        "      elif type(x) is list and not sector.input_is_list:\n",
        "        y = []\n",
        "        for x_i in x:\n",
        "          y.append(sector(x_i))\n",
        "      else:\n",
        "        y = sector(x)\n",
        "      x = y\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUHMaqhOCnQN"
      },
      "source": [
        "## Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdNNMACWCqiH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Type, Any\n",
        "\n",
        "class DataLoaderCreator(PipelineSector):\n",
        "  def __init__(\n",
        "    self,\n",
        "    DS: Type[Dataset],\n",
        "    ds_kwargs: dict[str, Any],\n",
        "    dl_kwargs: dict[str, Any],\n",
        "    with_eval = True,\n",
        "    exclude_train = [],\n",
        "    exclude_eval = []\n",
        "  ):\n",
        "    super().__init__(ignore_input=True)\n",
        "\n",
        "    self.DS = DS\n",
        "    self.ds_kwargs = ds_kwargs\n",
        "    self.dl_kwargs = dl_kwargs\n",
        "    self.with_eval = with_eval\n",
        "    self.exclude_train = exclude_train\n",
        "    self.exclude_eval = exclude_eval\n",
        "\n",
        "  def __call__(self, root_dir:str):\n",
        "    dataloaders = []\n",
        "\n",
        "    if root_dir is None:\n",
        "      root_dir = self.ds_kwargs.get('root_dir')\n",
        "      del self.ds_kwargs['root_dir']\n",
        "\n",
        "    dataset = self.DS(root_dir, exclude_list=self.exclude_train, **self.ds_kwargs)\n",
        "    train_loader = DataLoader(dataset, **self.dl_kwargs)\n",
        "    dataloaders.append(train_loader)\n",
        "\n",
        "\n",
        "    if self.with_eval:\n",
        "      dataset = self.DS(root_dir, exclude_list=self.exclude_eval, **self.ds_kwargs)\n",
        "      eval_loader = DataLoader(dataset, **self.dl_kwargs)\n",
        "      dataloaders.append(eval_loader)\n",
        "\n",
        "\n",
        "    return dataloaders if self.with_eval else dataloaders[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCqAeofUJvrQ"
      },
      "source": [
        "## Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqWmRgL6JyQ_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from typing import Type, Any\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ModelTrainer(PipelineSector):\n",
        "  def __init__(\n",
        "      self,\n",
        "      Model: Type[nn.Module],\n",
        "      device: torch.device,\n",
        "      chunk_size:int,\n",
        "      slide_size:int,\n",
        "      model_path:str,\n",
        "      save_path:str,\n",
        "      model_name:str,\n",
        "      patience,\n",
        "      num_epochs,\n",
        "      thresh,\n",
        "      kwargs: dict[str, Any]\n",
        "  ):\n",
        "    super().__init__(input_is_list=True)\n",
        "\n",
        "    self.model = Model(**kwargs)\n",
        "    self.device = device\n",
        "    self.chunk_size = chunk_size\n",
        "    self.slide_size = slide_size\n",
        "    self.save_path = save_path\n",
        "    self.model_name = model_name\n",
        "    self.patience = patience\n",
        "    self.num_epochs = num_epochs\n",
        "    self.thresh = thresh\n",
        "\n",
        "    if model_path is not None:\n",
        "      state_dict = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
        "      self.model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "  def initialize_metrics(self):\n",
        "    return {\n",
        "        'train_loss': [],\n",
        "        'train_recall': [],\n",
        "        'train_precision': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_iou': [],\n",
        "        'val_loss': [],\n",
        "        'val_recall': [],\n",
        "        'val_precision': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_f1': [],\n",
        "        'val_iou': []\n",
        "    }\n",
        "\n",
        "  def __call__(self, dataloaders: list[DataLoader]):\n",
        "    save_path_ = os.path.join(self.save_path, self.model_name)\n",
        "    last_model_path = os.path.join(self.save_path, 'last_model.pth')  # Path to save the last model\n",
        "\n",
        "    train_loader = dataloaders[0]\n",
        "    val_loader = dataloaders[1] if len(dataloaders)>1 else None\n",
        "\n",
        "\n",
        "    self.model.to(self.device)\n",
        "\n",
        "    pos_weight = torch.tensor([9.0]).to(self.device)  # Weight for the positive class (1)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # Use BCEWithLogitsLoss for stability\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    # Metrics storage\n",
        "    metrics = self.initialize_metrics()\n",
        "\n",
        "    best_train_iou = float('-inf')  # Initialize best iou score for train\n",
        "    best_val_iou = float('-inf')  # Initialize best iou score for validation\n",
        "\n",
        "    best_model_state = None  # To store the best model state\n",
        "    epochs_no_improve = 0  # Counter for epochs without improvement\n",
        "\n",
        "    for epoch in range(self.num_epochs):\n",
        "        # --- Training Phase ---\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        all_train_labels = []\n",
        "        all_train_preds = []\n",
        "\n",
        "        for i, ((img_before, img_current, img_after), labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{self.num_epochs}')):\n",
        "            img_before = img_before.to(self.device)\n",
        "            img_current = img_current.to(self.device)\n",
        "            img_after = img_after.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(img_before, img_current, img_after)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            all_train_labels.extend(labels.cpu().numpy())  # Store labels for metric calculation\n",
        "            all_train_preds.extend((outputs.cpu().detach().numpy() > self.thresh).astype(float))  # Binarize predictions\n",
        "\n",
        "            if (i + 1) % 10 == 0:  # Print every 10 batches\n",
        "                print(f'Epoch [{epoch + 1}/{self.num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}')\n",
        "\n",
        "        # Calculate training metrics for the epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_recall = recall_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "        epoch_precision = precision_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
        "        epoch_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
        "        epoch_f1 = 2 * (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall + 1e-7)  # f1 calculation\n",
        "\n",
        "        intersection = np.logical_and(all_train_labels, all_train_preds).sum()\n",
        "        union = np.logical_or(all_train_labels, all_train_preds).sum()\n",
        "        epoch_iou = intersection / float(union) if union > 0 else 0\n",
        "\n",
        "        # Store training metrics\n",
        "        metrics['train_loss'].append(epoch_loss)\n",
        "        metrics['train_recall'].append(epoch_recall)\n",
        "        metrics['train_precision'].append(epoch_precision)\n",
        "        metrics['train_accuracy'].append(epoch_accuracy)\n",
        "        metrics['train_f1'].append(epoch_f1)\n",
        "        metrics['train_iou'].append(epoch_iou)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{self.num_epochs}] complete. Train Loss: {epoch_loss:.4f}, Train Recall: {epoch_recall:.4f}, Train Precision: {epoch_precision:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Train f1: {epoch_f1:.4f}, Train iou: {epoch_iou:.4f}')\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        if val_loader:\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            all_val_labels = []\n",
        "            all_val_preds = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for (img_before, img_current, img_after), labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                    img_before = img_before.to(self.device)\n",
        "                    img_current = img_current.to(self.device)\n",
        "                    img_after = img_after.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "\n",
        "                    outputs = self.model(img_before, img_current, img_after)\n",
        "                    loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    all_val_labels.extend(labels.cpu().numpy())  # Store labels for metric calculation\n",
        "                    all_val_preds.extend((outputs.cpu().detach().numpy() > 0.5).astype(float))  # Binarize predictions\n",
        "\n",
        "            # Calculate validation metrics for the epoch\n",
        "            val_loss /= len(val_loader)\n",
        "            val_recall = recall_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n",
        "            val_precision = precision_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n",
        "            val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
        "            val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall + 1e-7)\n",
        "\n",
        "            intersection = np.logical_and(all_val_labels, all_val_preds).sum()\n",
        "            union = np.logical_or(all_val_labels, all_val_preds).sum()\n",
        "            val_iou = intersection / float(union) if union > 0 else 0\n",
        "\n",
        "            # Store validation metrics\n",
        "            metrics['val_loss'].append(val_loss)\n",
        "            metrics['val_recall'].append(val_recall)\n",
        "            metrics['val_precision'].append(val_precision)\n",
        "            metrics['val_accuracy'].append(val_accuracy)\n",
        "            metrics['val_f1'].append(val_f1)\n",
        "            metrics['val_iou'].append(val_iou)\n",
        "\n",
        "            print(f'Validation complete. Val Loss: {val_loss:.4f}, Val Recall: {val_recall:.4f}, Val Precision: {val_precision:.4f}, Val Accuracy: {val_accuracy:.4f}, Val f1: {val_f1:.4f}, Val iou: {val_iou:.4f}')\n",
        "\n",
        "        # Save metrics and models\n",
        "        history_path = os.path.join(self.save_path, f'{self.model_name[:-4]}.pkl')\n",
        "        with open(history_path, 'wb') as f:\n",
        "            pickle.dump(metrics, f)\n",
        "\n",
        "\n",
        "        if epoch_iou > best_train_iou:  # Train IoU\n",
        "            best_train_iou = epoch_iou\n",
        "            best_model_state = self.model.state_dict()  # Save the best model's state dict based on train IoU\n",
        "            torch.save(best_model_state, os.path.join(self.save_path, f\"best_model_train_iou.pth\"))  # Save with train IoU tag\n",
        "            print(f\"New best model found at epoch {epoch+1} with train IoU score: {epoch_iou:.4f}. Saving model.\")\n",
        "\n",
        "        # Early stopping and best model saving based on validation iou\n",
        "        if val_loader and val_iou > best_val_iou:\n",
        "            best_val_iou = val_iou\n",
        "            best_model_state = self.model.state_dict()  # Save the best model's state dict\n",
        "            torch.save(best_model_state, save_path_)  # Save the best model\n",
        "            print(f\"New best model found at epoch {epoch+1} with iou score: {val_iou:.4f}. Saving model.\")\n",
        "            epochs_no_improve = 0  # Reset the counter if iou improved\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"No improvement in iou score for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "        # Save the last model at the end of each epoch\n",
        "        torch.save(self.model.state_dict(), last_model_path)\n",
        "        print(f\"Last model saved at epoch {epoch+1}.\")\n",
        "\n",
        "        if epochs_no_improve >= self.patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. Best iou score: {best_val_iou:.4f}\")\n",
        "            break\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0H_6Ns1jkUH"
      },
      "source": [
        "## Model Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOrhSaRqjnZi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Type, Any\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ModelLoader(PipelineSector):\n",
        "  def __init__(\n",
        "      self,\n",
        "      Model: Type[nn.Module],\n",
        "      model_path:str,\n",
        "      device: torch.device,\n",
        "      chunk_size:int,\n",
        "      slide_size:int,\n",
        "      thresh:float,\n",
        "      kwargs: dict[str, Any]\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = Model(**kwargs)\n",
        "    self.device = device\n",
        "    self.chunk_size = chunk_size\n",
        "    self.slide_size = slide_size\n",
        "    self.thresh=thresh\n",
        "\n",
        "    state_dict = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
        "\n",
        "    self.model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "  def pad_beg_end(self, L:list, length:int=15):\n",
        "    # Create the padding list\n",
        "    z = [0.0] * length\n",
        "\n",
        "    # Create a copy of the list\n",
        "    L_copy = L.copy()\n",
        "\n",
        "    # Insert padding at the beginning and append it at the end\n",
        "    L_copy.insert(0, z)\n",
        "    L_copy.append(z)\n",
        "\n",
        "    # Return the padded copy\n",
        "    return L_copy\n",
        "\n",
        "\n",
        "  def average_lists(self, list1:list, list2:list):\n",
        "      \"\"\"A helper function to average two lists element-wise.\"\"\"\n",
        "      return [(a + b) / 2 for a, b in zip(list1, list2)]\n",
        "\n",
        "\n",
        "  def sliding_window_operation(self, data:list):\n",
        "      # Create a deep copy of the data to avoid changing the original\n",
        "      data_copy = copy.deepcopy(data)\n",
        "\n",
        "      # Ensure we have at least one window to process\n",
        "      if len(data_copy) < 2:\n",
        "          raise ValueError(\"Data must contain at least two elements.\")\n",
        "\n",
        "      i = 0\n",
        "      while len(data_copy) >= 2:\n",
        "          # Get the current window\n",
        "          current_window = data_copy[0:2]\n",
        "          # print('Current:', current_window)\n",
        "\n",
        "          len_0 = len(current_window[0])\n",
        "          len_1 = len(current_window[1])\n",
        "\n",
        "          window_0_last = current_window[0][-self.chunk_size:]\n",
        "          window_1_first = current_window[1][-self.chunk_size:]\n",
        "          average_first = self.average_lists(window_0_last, window_1_first)\n",
        "\n",
        "\n",
        "          # First\n",
        "          window_0_first = current_window[0][:(len_0 - self.chunk_size)]\n",
        "          window_1_last = current_window[0][-self.slide_size:]\n",
        "\n",
        "          new = window_0_first + average_first + window_1_last\n",
        "\n",
        "          # Replace the two elements with the new list\n",
        "          data_copy[0: 2] = [new]\n",
        "\n",
        "      return data_copy\n",
        "\n",
        "\n",
        "  def __call__(self, dataloader: DataLoader):\n",
        "    list_of_outputs = []\n",
        "\n",
        "    self.model.eval()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "    for i,(img_before, img_current, img_after) in enumerate(tqdm(dataloader, desc='Making Prediction')):\n",
        "      img_before = img_before.to(self.device)\n",
        "      img_current = img_current.to(self.device)\n",
        "      img_after = img_after.to(self.device)\n",
        "\n",
        "      ot = self.model(img_before, img_current, img_after)\n",
        "      ot = ot.flatten()\n",
        "      ot = ot.detach().cpu().numpy()\n",
        "      ot = list(ot)\n",
        "      list_of_outputs.append(ot)\n",
        "\n",
        "\n",
        "    padded_list = self.pad_beg_end(list_of_outputs, self.chunk_size)\n",
        "    labels = np.array(self.sliding_window_operation(padded_list), dtype=float).flatten()\n",
        "\n",
        "    labels[labels > self.thresh] = 1\n",
        "    labels[labels <= self.thresh] = 0\n",
        "\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-2zdPZ9Xo19"
      },
      "source": [
        "# Train Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cluw5LtNXwSU"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize the Mel spectrogram\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "])\n",
        "\n",
        "pipe = Pipeline()\n",
        "\n",
        "pipe.append(WavLoader())\n",
        "pipe.append(ChunksCreator(\n",
        "    'chunks',\n",
        "    NoiseCancalation(),\n",
        "    MelSpectrogram(),\n",
        "    exist_ok=False\n",
        "))\n",
        "pipe.append(DataLoaderCreator(\n",
        "    MelSpectrogramDataset,\n",
        "    {\n",
        "        'chunk_size': 15,\n",
        "        'sample_rate': 44100,\n",
        "        'transform': data_transform,\n",
        "        'root_dir': 'chunks'\n",
        "    },\n",
        "    {\n",
        "        'batch_size': 32,\n",
        "        'shuffle': False\n",
        "    }\n",
        "))\n",
        "pipe.append(ModelTrainer(\n",
        "    Model=MultiInputResNet,\n",
        "    device='cpu',\n",
        "    chunk_size=15,\n",
        "    slide_size=5,\n",
        "    model_path=None,\n",
        "    save_path='.',\n",
        "    model_name='best.pth',\n",
        "    patience=3,\n",
        "    num_epochs=10,\n",
        "    thresh=0.193,\n",
        "    kwargs={\n",
        "        'num_labels': 15\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NPztCuA0Lv0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjNmoFo4XuKY"
      },
      "source": [
        "# Test Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFwqBf3nXwub"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize the Mel spectrogram\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "])\n",
        "\n",
        "pipe = Pipeline()\n",
        "\n",
        "# pipe.append(WavDownloader(\n",
        "#     root_dir='test',\n",
        "#     file_names=[\n",
        "#         'S2.wav',\n",
        "#         'S6.wav',\n",
        "#         'S12.wav'\n",
        "#     ]\n",
        "# ))\n",
        "pipe.append(WavLoader())\n",
        "pipe.append(ChunksCreator(\n",
        "    'chunks',\n",
        "    NoiseCancalation(),\n",
        "    MelSpectrogram(),\n",
        "    exist_ok=False,\n",
        "    with_labels=False\n",
        "))\n",
        "pipe.append(DataLoaderCreator(\n",
        "    DS=MelSpectrogramDataset,\n",
        "    with_eval=False,\n",
        "    ds_kwargs={\n",
        "        'chunk_size': 15,\n",
        "        'sample_rate': 44100,\n",
        "        'transform': data_transform,\n",
        "        'root_dir': 'chunks'\n",
        "    },\n",
        "    dl_kwargs={\n",
        "        'batch_size': 32,\n",
        "        'shuffle': False\n",
        "    }\n",
        "))\n",
        "pipe.append(ModelLoader(\n",
        "    Model=MultiInputResNet,\n",
        "    device='cpu',\n",
        "    chunk_size=15,\n",
        "    slide_size=5,\n",
        "    model_path='/content/drive/MyDrive/AIA/public AIA/fold_3_kaggle_val.pth',\n",
        "    thresh=0.193,\n",
        "    kwargs={\n",
        "        'num_labels': 15\n",
        "    }\n",
        "))\n",
        "pipe.append(Labeler(\n",
        "    min_length_ones=10\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy2qXgzu75ef",
        "outputId": "213941b2-8e65-433f-8fd3-a12555bcb62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WavLoader] Is Running\n",
            "[ChunksCreator] Is Running\n",
            "[DataLoaderCreator] Is Running\n",
            "[ModelLoader] Is Running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Making Prediction: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Labeler] Is Running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# pipe.run([\n",
        "#     'http://dl.iaievent.com/Challenge1/Test_S2.wav',\n",
        "#     'http://dl.iaievent.com/Challenge1/Test_S6.wav',\n",
        "#     'http://dl.iaievent.com/Challenge1/Test_S12.wav'\n",
        "# ])\n",
        "res = pipe.run('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW1kmwJqA1yp",
        "outputId": "9049c399-95ce-4623-b5c5-b7f894201485"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['0:00', '0:00']]"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wDPca9r7yJm7",
        "9SD4yF1PZRrP",
        "AX7YMGU4LfYQ",
        "LgMiIp31Qf1s",
        "Yq4EtG8OZnlQ",
        "iltLfTR9jUr9",
        "PjQk6lTyjYXf",
        "K2tuqSiRHD6t",
        "TBKSw0L-a5qf",
        "JJXfERLq42_D",
        "Y7F7cMHmjddu",
        "2zELRlIk8vI1",
        "TUHMaqhOCnQN",
        "H0H_6Ns1jkUH",
        "xz3xNXf16EAc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}